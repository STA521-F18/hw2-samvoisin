---
title: "HW2 STA521 Fall18"
author: 'Sam Voisin (netid: psv6, github: samvoisin)'
date: "Due September 23, 2018 5pm"
output:
  html_document:
  html_notebook: default
  pdf_document: default

  
---

add back to top after done!!!

pdf_document: default
  html_document:
    df_print: paged

## Backgound Reading

Readings: Chapters 3-4 in Weisberg Applied Linear Regression


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This exercise involves the UN data set from `alr3` package. Install `alr3` and the `car` packages and load the data to answer the following questions adding your code in the code chunks.  Please add appropriate code to the chunks to suppress messages and warnings as needed once you are sure the code is working properly and remove instructions if no longer needed. Figures should have informative captions. Please switch the output to pdf for your final version to upload to Sakai. **Remove these instructions for final submission**


## Exploratory Data Analysis

0.  Preliminary read in the data.  After testing, modify the code chunk so that output, messages and warnings are suppressed.  *Exclude text from final*

```{r data, include = FALSE}
library(alr3)
library(knitr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(GGally)
library(gridExtra)
library(car)
data(UN3, package="alr3")
#help(UN3)

print_na_cols <- function(df) {
  # name data frame columns containing NA values
  na_cols <- c()
  
  for (name in names(UN3)) {
    if (any(is.na(UN3[name]))) {
      na_cols <- c(na_cols, name)
    }
  }
  print(na_cols)
}

```


1. Create a summary of the data.  How many variables have missing data?  Which are quantitative and which are qualtitative?

Using the `print_na_cols` function I created, we can see that `ModernC`, `Change`, `PPgdp`, `Frate`, `Pop`, and `Fertility` all contain `NA` values. This leaves Purban as the only column with no NA values.

All data is a quantitative measurement in some way. `ModernC`, `Change`, `Frate`, and `Purban` are percentages. `Pop` and `Fertility` are counts. `PPgdp` is a continuous quantitative variable (minus the fact that we can't divide beyond a penny).

```{r Question1, echo = FALSE}

str(UN3)

print("Columns with NA values:")
print_na_cols(UN3)


```

2. What is the mean and standard deviation of each quantitative predictor?  Provide in a nicely formatted table.

```{r Question2, echo = FALSE}

col_mean <- round(sapply(UN3, mean, na.rm = TRUE), 2)
col_sd <- round(sapply(UN3, sd, na.rm = TRUE), 2)

kable(
  cbind(col_mean, col_sd),
  col.names = c("Mean", "St. Dev.")
  )

```


3. Investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots
highlighting the relationships among the predictors. Comment
on your findings regarding trying to predict `ModernC` from the other variables.  Are there potential outliers, nonlinear relationships or transformations that appear to be needed based on your graphical EDA?

  As a first step, I used the ggpairs to create a matrix of all of the variables. From the plot matrix, It is clear that there are two outliers in `Pop`. These points represent China and India. The next observation I made from the plot matrix is the relationship `Fertility` has with several of the variables. It seems that `Fertility` has a slightly exponential relationship with both `ModernC` and `Change` although in opposite directions. This relationship becomes more extreme for `PPgdp`. Unsurprisingly, the points of `PPgdp` skew right as this metric cannot be negative. The non-normality of this relationship will likely have to be corrected via log transformation. Finally, `Purban` has an almost linear, positive relationship with `ModernC`.

  The red lines in three of the plots below are "local regression" (loess) lines useful for visualizing the relationship between the variables.

```{r Question3, echo = FALSE, warn = FALSE, fig.width= 14, fig.height= 10}

UN3_rmna <- UN3[complete.cases(UN3), ]

ggpairs(UN3_rmna)
  
# create Pop vs ModernC plot
PopModc <- ggplot(data = UN3_rmna, aes(x = Pop, y = ModernC)) + 
  geom_point(size = 2.5) +
  ggtitle("Population vs Modern Contraception %") +
  labs(x = "Population (000s)",
       y = "% Access to Contraception") +
  geom_text(aes(label = ifelse(Pop > 1000000, row.names(UN3_rmna),''),
                hjust = 1,
                vjust = 2)) +
  theme(plot.title = element_text(size = 18, face = "bold"),
        axis.title = element_text(size = 17),
          axis.text = element_text(size=18))


# create PPgdp vs ModernC plot
PPgdpModc <- ggplot(data = UN3_rmna, aes(x = PPgdp, y = ModernC)) + 
  geom_point(size = 2.5) +
  geom_smooth(color = "red", se = FALSE) +
  ggtitle("Per Capita GDP vs Modern Contraception %") +
  labs(x = "Per Capita GDP (USD)",
       y = "% Access to Contraception") +
  theme(plot.title = element_text(size = 18, face = "bold"),
        axis.title = element_text(size = 17),
          axis.text = element_text(size=18))
  

# create ModernC vs Fertility plot
ModcFert <- ggplot(data = UN3_rmna, aes(x = ModernC, y = Fertility)) + 
  geom_point(size = 2.5) +
  geom_smooth(color = "red", se = FALSE) +
  ggtitle("Modern Contraception % vs Fertility Rate") +
  labs(x = "% Access to Contraception",
       y = "Expected Fertility Rate (# per female)") +
  theme(plot.title = element_text(size = 18, face = "bold"),
        axis.title = element_text(size = 17),
          axis.text = element_text(size=18))


# create Purban vs ModernC
PurbModc <- ggplot(data = UN3_rmna, aes(x = Purban, y = ModernC)) + 
  geom_point(size = 2.5) +
  geom_smooth(color = "red", se = FALSE) +
  ggtitle("Urban Population % vs Modern Contraception %") +
  labs(x = "% of Population in Cities",
       y = "% Access to Contraception") +
  theme(plot.title = element_text(size = 18, face = "bold"),
        axis.title = element_text(size = 17),
        axis.text = element_text(size=18))

grid.arrange(PopModc, PPgdpModc, ModcFert, PurbModc, ncol = 2)

```

## Model Fitting

4.  Use the `lm()` function to perform a multiple linear regression with `ModernC` as the response and all other variables as the predictors, using the formula `ModernC ~ .`, where the `.` includes all remaining variables in the dataframe.  Create  diagnostic residual plot from the linear model object and comment on results regarding assumptions.  How many observations are used in your model fitting?

  From the below residual plots we can see a few issues regarding the three assumptions for linear models. First, it seems that the constant variance assumption is incorrect. We can see this in the Residuals vs Fitted and the Scale-Location graphs. Both seem to fan out slightly as the x-axis becomes larger. From the Normal Q-Q plot we can see that the normality assumption begins to break down at the positive extreme of the normal distribution. Points that theoretically should be in the positive second standard deviation seem to be closer to the first (i.e. the residuals seem to be skewed right). It is worth noting that China and India, while having high leverage, are not outside the contours of Cook's Distance.  
  
125 observations were used in fitting the model.

```{r Question4, echo = FALSE, fig.width= 14, fig.height= 10}

PreTMod <- lm(ModernC ~ ., data = UN3_rmna)

par(mfrow=c(2,2))
plot(PreTMod, ask=F)

outlierTest(PreTMod)

```

5. Examine added variable plots `car::avPlot` or `car::avPlots`  for your model above. Are there any plots that suggest that transformations are needed for any of the terms in the model? Describe. Is it likely that any of the localities are influential for any of the terms?  Which localities?  Which terms?

As mentioned previously, China and Inda are the highest leverage values in the third graph of column two, ModernC | others vs Pop | others. However the result of a Bonferroni corrected T test of the studentized residuals does not result in any t-statistics with a p-value significantly small to reject $H_0: \mu_i = E[Y|X = x_i]$.

```{r Question5, echo = FALSE, fig.width= 14, fig.height= 10}

avPlots(PreTMod)

outlierTest(PreTMod)

Cook_Dist <- cooks.distance(PreTMod)

Cook_DF <- data.frame(China = round(Cook_Dist["China"], 4),
                      India = round(Cook_Dist["India"], 4),
                      row.names = "Cook's Dist.")

kable(Cook_DF)

```

6.  Using the Box-Tidwell  `car::boxTidwell` or graphical methods find appropriate transformations of the predictor variables to be used as predictors in the linear model.  If any predictors are negative, you may need to transform so that they are non-negative.  Describe your method and  the resulting transformations.



```{r Question6, fig.width= 14, fig.height= 10}

boxTidwell(ModernC ~ PPgdp + Pop + Fertility,
           other.x = ~ Change + Purban + Frate,
           data = UN3_rmna)

PostXTMod <- lm(ModernC ~ Change + log(PPgdp) +
                  Frate + log(Pop) +
                  Fertility + Purban,
                data = UN3_rmna)

avPlots(PostXTMod)

```

7. Given the selected transformations of the predictors, select a transformation of the response using `MASS::boxcox` or `car::boxCox` and justify.


```{r Question7, echo = FALSE}

boxCox(PostXTMod)

PostYTMod <- lm(sqrt(ModernC) ~ Change + log(PPgdp) +
                  Frate + log(Pop) +
                  Fertility + Purban,
                data = UN3_rmna)

```

8.  Fit the regression using the transformed variables.  Provide residual plots and added variables plots and comment.  If you feel that you need additional transformations of either the response or predictors, repeat any steps until you feel satisfied.

```{r Question8, echo = FALSE, fig.width= 14, fig.height= 10}

avPlots(PostYTMod)

par(mfrow=c(2,2))
plot(PreTMod, ask=F)


```

9. Start by finding the best transformation of the response and then find transformations of the predictors.  Do you end up with a different model than in 8?

The result below shows that the interval for the maximum likelihood estimator for lambda using the BoxCox procedure prior to the BoxTidwell procedure is very close if not identical to same procedure in reverse order. 


```{r Question9}

boxCox(PreTMod)
boxTidwell(ModernC ~ PPgdp + Pop + Fertility,
           other.x = ~ Change + Purban + Frate,
           data = UN3_rmna)

```

10.  Are there any outliers or influential points in the data?  Explain.  If so, refit the model after removing any outliers and comment on residual plots.


```{r Question10, echo = FALSE, fig.width= 14, fig.height= 10}

# outlierTest(PostYTMod)

ggplot(PostYTMod, aes(x = hatvalues(PostYTMod), y = rstandard(PostYTMod))) + 
  geom_point(size = 2.5) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  ggtitle("Standardized Residuals vs Leverage") +
  labs(x = "Leverage",
       y = "Standardized Residuals") +
  theme(plot.title = element_text(size = 20, face = "bold"),
        axis.title = element_text(size = 20),
        axis.text = element_text(size=18))

Cook_Dist <- cooks.distance(PostYTMod)

Cook_DF <- data.frame(China = round(Cook_Dist["China"], 4),
                      India = round(Cook_Dist["India"], 4),
                      row.names = "Cook's Dist.")

kable(Cook_DF)

```

## Summary of Results

11. For your final model, provide summaries of coefficients with 95% confidence intervals in a nice table with interpretations of each coefficient.  These should be in terms of the original units!


```{r Question11}

SummDF <- data.frame(LowB = round(confint.lm(PostYTMod)[, 1], 2),
                     HighB = round(confint.lm(PostYTMod)[, 2], 2),
                     Betas = round(PostYTMod$coefficients, 2),
                     row.names = names(PostYTMod$coefficients))

kable(SummDF, col.names = c("Coef. Est.", "2.5%", "97.5%"))

```


12. Provide a paragraph summarizing your final model and findings suitable for the US envoy to the UN after adjusting for outliers or influential points.   You should provide a justification for any case deletions in your final model


```{r Question12}



```


## Methodology

    
13. Prove that the intercept in the added variable scatter plot will always be zero.  _Hint:  use the fact that if $H$ is the project matrix which contains a column of ones, then $1_n^T (I - H) = 0$.  Use this to show that the sample mean of residuals will always be zero if there is an intercept._


14. For multiple regression with more than 2 predictors, say a full model given by `Y ~ X1 + X2 + ... Xp`   we create the added variable plot for variable `j` by regressing `Y` on all of the `X`'s except `Xj` to form `e_Y` and then regressing `Xj` on all of the other X's to form `e_X`.  Confirm that the slope in a manually constructed added variable plot for one of the predictors  in Ex. 10 is the same as the estimate from your model. 
